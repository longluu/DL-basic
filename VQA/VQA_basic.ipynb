{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VQA-basic.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Od0Q5SzEzd8C",
        "DCKOXMDwTvtE",
        "AA3D5qNfVWSs",
        "WytdlCrO6iaq",
        "02reDBJKtCk8"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9thHjd3JomO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBEobjSHL4kr",
        "colab_type": "code",
        "outputId": "ca8f1603-390e-46d1-9e37-a8227c975837",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipython-autotime in /usr/local/lib/python3.6/dist-packages (0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBeg8kixJ7V2",
        "colab_type": "code",
        "outputId": "5cd1b0db-29d5-4cae-c5a7-68b98cb422e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.55 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Egzi3phdygmE",
        "colab_type": "code",
        "outputId": "2f37c250-0879-4603-82f5-0957f0ff56a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "# Check GPU\n",
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Apr  1 15:36:22 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P8    32W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "time: 1.06 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77VYYVBPu5Dx",
        "colab_type": "text"
      },
      "source": [
        "### Download and preprocess the starter CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YS7gkk-pHZZ7",
        "colab_type": "code",
        "outputId": "1d54a84d-ac14-444b-c8df-01e1231d037e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Download the text file\n",
        "def download_if_missing(url, target, extract=True):\n",
        "  if os.path.exists(target):\n",
        "    return target\n",
        "  return tf.keras.utils.get_file(target, origin=url, extract=extract)\n",
        "\n",
        "colab_root = \"/content/\"\n",
        "csv_path = os.path.join(colab_root, \"starter2.csv\")\n",
        "download_if_missing(\"https://storage.googleapis.com/applied-dl/mini-vqa/starter2.csv\",\n",
        "                     csv_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/starter2.csv'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "stream",
          "text": [
            "time: 16.1 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCCzA0CXb27r",
        "colab_type": "code",
        "outputId": "79274636-2819-40f0-fcb4-fad21309d1c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "# Process the text file\n",
        "# Load the text data\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "\n",
        "text_data = pd.read_csv(\"starter2.csv\", header=None)\n",
        "text_data.head()\n",
        "\n",
        "# Extract different components\n",
        "questions = text_data[0]\n",
        "answers = text_data[1]\n",
        "images_name = text_data[2]\n",
        "\n",
        "# Convert answers to numeric values\n",
        "answers_numeric = np.zeros((answers.size,))\n",
        "answers_numeric[answers=='yes'] = 1\n",
        "answers_numeric[answers=='no'] = 0\n",
        "for answer, answer_numeric in zip(answers[:10], answers_numeric):\n",
        "    print(answer + \" \" + str(answer_numeric))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "yes 1.0\n",
            "yes 1.0\n",
            "yes 1.0\n",
            "yes 1.0\n",
            "no 0.0\n",
            "yes 1.0\n",
            "yes 1.0\n",
            "yes 1.0\n",
            "yes 1.0\n",
            "no 0.0\n",
            "time: 51.2 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKNgLhjovm5u",
        "colab_type": "text"
      },
      "source": [
        "## Notebook two: Preprocessing and training\n",
        "Create a new notebook for this. At the start, download your starter CSV and thumbnails.zip.\n",
        "\n",
        "The starter code below assumes you have created a zip file called \"thumbnails.zip\" containing only the resized images from the COOC training set mentioned in the starter CSV, and that you can download it from a URL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIofbvodzuyl",
        "colab_type": "code",
        "outputId": "a4e327c2-757b-4c47-9271-124e92af57d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n",
            "time: 31.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_pGuZWXx4vE",
        "colab_type": "text"
      },
      "source": [
        "### Convert image names to absolute paths\n",
        "\n",
        "Next, for convenience, it may be helpful to update the images column in the starter CSV from filenames ```COCO_train2014_000000320111.jpg``` to absolute paths ```/content/images/COCO_train2014_000000320111.jpg```. This will save you some code down the road when it comes time to open them up (you won't need to worry about the relationship between where the starter CSV file is stored, and your images folder)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfT0fWTZg02N",
        "colab_type": "code",
        "outputId": "ce47af6c-2ff5-4133-dd67-c3177cf2905c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "root_dir = \"/content/drive/My Drive\"\n",
        "!unzip -q \"/content/drive/My Drive/activation.zip\" -d  \"/content/\"\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 5min 51s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9lbPJyvxv6O",
        "colab_type": "code",
        "outputId": "b9b906e4-d659-4f14-c0dc-8f96133e724b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# # Convert image name to full path and check if all images exist\n",
        "# image_folder = \"/content/drive/My Drive/SelectImage/\"\n",
        "# image_path = []\n",
        "# image_not_exist = []\n",
        "# for index, image_name in enumerate(images_name):\n",
        "#     image_path.append(image_folder + str(image_name))\n",
        "#     if not os.path.exists(image_path[index]):\n",
        "#       image_not_exist.append(image_path[index])\n",
        "\n",
        "# print(\"Number of non-existent images: \" + str(len(image_not_exist)))\n",
        "# print(\"Number of loaded images: \" + str(len(image_path)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.52 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pEDLDYvQGMv",
        "colab_type": "code",
        "outputId": "b20ab731-202c-4768-b741-e05f04784e28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Convert image name to full path and check if all activation exist\n",
        "activation_folder = \"/content/activation/\"\n",
        "image_path = []\n",
        "image_not_exist = []\n",
        "for index, image_name in enumerate(images_name):\n",
        "    image_path.append(activation_folder + str(image_name) + \".npy\")\n",
        "    if not os.path.exists(image_path[index]):\n",
        "      image_not_exist.append(image_path[index])\n",
        "\n",
        "print(\"Number of non-existent images: \" + str(len(image_not_exist)))\n",
        "print(\"Number of loaded images: \" + str(len(image_path)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of non-existent images: 0\n",
            "Number of loaded images: 20000\n",
            "time: 4.85 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXRFRLc4GszP",
        "colab_type": "code",
        "outputId": "f6d35771-5f4b-42cd-ef8b-e5ad73969142",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Check a few images and questions\n",
        "# import numpy as np\n",
        "# from matplotlib import pyplot as plt\n",
        "\n",
        "# n_image_check = 20\n",
        "# index_check = np.random.randint(0, images_name.size, (n_image_check,))\n",
        "# fig = plt.figure(figsize=(23, 20))\n",
        "# n_col = 5\n",
        "# for count, index in enumerate(index_check):\n",
        "#     # Original\n",
        "#     original_image = plt.imread(image_path[index])\n",
        "#     ax = fig.add_subplot(n_image_check/n_col, n_col, count+1)\n",
        "#     plt.imshow(original_image)\n",
        "#     ax.set_title(questions[index] + ': ' + answers[index])\n",
        "#     ax.axes.get_xaxis().set_visible(False)\n",
        "#     ax.axes.get_yaxis().set_visible(False)  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 968 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqWvyk_54MEB",
        "colab_type": "text"
      },
      "source": [
        "### Shuffle and create a test set\n",
        "For this assignment, you do not need to download the COCO validation or test sets. Instead, shuffle your starter CSV, and use some of the rows as a test set (say, 4,000 rows). Move them to a separate CSV or data structure, and revisit them later. I realize some of the same images may appear in the train and test set (although with different questions). For this assignment, that's okay. If you prefer, you can write code to ensure the test set has unique images that do not appear in the train set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUeKcg8H4OH2",
        "colab_type": "code",
        "outputId": "dfb8d8e5-0f5d-4ed3-8440-41f5313efcbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Pick random 4000 samples for test and the rest for training\n",
        "n_test = 4000\n",
        "index_shuffle = np.arange(len(image_path))\n",
        "np.random.shuffle(index_shuffle)\n",
        "test_image_path = [image_path[index] for index in index_shuffle[:n_test]]\n",
        "test_answer_numeric = answers_numeric[index_shuffle[:n_test]]\n",
        "test_questions = list(questions[index_shuffle[:n_test]])\n",
        "\n",
        "full_train_image_path = [image_path[index] for index in index_shuffle[n_test:]]\n",
        "full_train_answer_numeric = answers_numeric[index_shuffle[n_test:]]\n",
        "full_train_questions = list(questions[index_shuffle[n_test:]])\n",
        "\n",
        "print(\"Size of test set: \" + str(len(test_image_path)) + \",  \" + str(test_answer_numeric.shape[0]) + \",  \" + str(len(test_questions)))\n",
        "print(\"Size of train set: \" + str(len(full_train_image_path)) + \",  \" + str(full_train_answer_numeric.shape[0]) + \",  \" + str(len(full_train_questions)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of test set: 4000,  4000,  4000\n",
            "Size of train set: 16000,  16000,  16000\n",
            "time: 32.4 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmSXbHqrfdFJ",
        "colab_type": "text"
      },
      "source": [
        "### Create training and validation sets\n",
        "You may want to do an 80:20 split on your balanced training set, giving you 16,000 training rows, and 4,000 validation rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMto3Q1rxcQ4",
        "colab_type": "code",
        "outputId": "1a26dd70-6d9e-4dff-fd49-f084e6609332",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Your code here\n",
        "# The scikit-learn utilities (train_test_split) are your friend\n",
        "n_train = int(len(full_train_image_path) * 0.8)\n",
        "index_shuffle = np.arange(len(full_train_image_path))\n",
        "np.random.shuffle(index_shuffle)\n",
        "train_image_path = [full_train_image_path[index] for index in index_shuffle[:n_train]]\n",
        "train_answer_numeric = full_train_answer_numeric[index_shuffle[:n_train]]\n",
        "train_questions = [full_train_questions[index] for index in index_shuffle[:n_train]]\n",
        "\n",
        "val_image_path = [full_train_image_path[index] for index in index_shuffle[n_train:]]\n",
        "val_answer_numeric = full_train_answer_numeric[index_shuffle[n_train:]]\n",
        "val_questions = [full_train_questions[index] for index in index_shuffle[n_train:]]\n",
        "\n",
        "print(\"Size of train set: \" + str(len(train_image_path)) + \",  \" + str(train_answer_numeric.shape[0]) + \",  \" + str(len(train_questions)))\n",
        "print(\"Size of validation set: \" + str(len(val_image_path)) + \",  \" + str(val_answer_numeric.shape[0]) + \",  \" + str(len(val_questions)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of train set: 12800,  12800,  12800\n",
            "Size of validation set: 3200,  3200,  3200\n",
            "time: 17.7 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od0Q5SzEzd8C",
        "colab_type": "text"
      },
      "source": [
        "### Verify your train and validation sets look as expected\n",
        "You cannot spend too much time exploring the data. When I'm developing code for something like this on my own, I often run methods to verify that these splits have the number of rows I expect, to display a few images from each, and to see stats on the class balance. I also write code to verify the splits contain *only* yes/no answers. You may be tired of coding defensively at this point, but I assure you (especially when working with new and increasingly complicated datasets, this effort **always** pays off)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEaPJjQuSouo",
        "colab_type": "code",
        "outputId": "d0c51434-99c3-4d16-8884-918683a1bc54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# # Check the proportion of split\n",
        "# print(\"Proportion of train: \" + str(y_train.shape[0] / full_train_answer_numeric.shape[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 5.82 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rXdrUZQz5f4",
        "colab_type": "code",
        "outputId": "94146def-d5ed-4633-8fd6-618f4c94e83e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# # Check the balance of answers in train and val sets\n",
        "# p_yes_train = sum(y_train==1) / y_train.shape\n",
        "# print(\"Proportion of yes in train sets: \" + str(p_yes_train))\n",
        "\n",
        "# p_yes_val = sum(y_val==1) / y_val.shape\n",
        "# print(\"Proportion of yes in train sets: \" + str(p_yes_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 2.99 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ITEBFcQRqdm",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Forward your images through InceptionV3, and cache activations to disk\n",
        "\n",
        "Rather than training a CNN from scratch for your VQA model, you'll begin by using activations from a pretrained model.  \n",
        "\n",
        "* Instead of forward each image repeatedly through the model while training (which will be slow), let's do that once and save the activations to disk. \n",
        "\n",
        "* We're saving them to disk (rather than keeping them in memory) to accomodate different sizes of datasets down the road.\n",
        "\n",
        "I've written a good deal of this code for you, but you should go through it and carefully understand how it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbMdRBX8TY5X",
        "colab_type": "code",
        "outputId": "9d38e235-0800-4311-faaf-06c82474997d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# # Create a feature extraction model.\n",
        "# # You should not need to modify this (though you may, if you'd like\n",
        "# # to use a model other than Inception).\n",
        "# image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
        "#                                                 weights='imagenet')\n",
        "# new_input = image_model.input\n",
        "# hidden_layer = image_model.layers[-1].output\n",
        "# image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 3.61 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW0drXDAQu3c",
        "colab_type": "code",
        "outputId": "8eddf2a4-c578-462c-f7ee-a77d802b4d29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# # A method to load an image off disk, and extract activations using \n",
        "# # the model above. You should not need to modify this.\n",
        "# def image_to_activations(image_path):\n",
        "#   img = tf.io.read_file(image_path)\n",
        "#   img = tf.image.decode_jpeg(img, channels=3)\n",
        "#   img = tf.image.resize(img, (299, 299))\n",
        "#   activations = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "#   return activations, image_path"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 5.06 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2_1Ili7y3Fj",
        "colab_type": "code",
        "outputId": "e86632fe-9578-4c27-dfed-801d1ff0cba5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# # You'll need to extract activations for every image in your train, validation,\n",
        "# # and test set. First, create a set of the absolute paths to all of these images \n",
        "# # (image_path_set). Populate this with the absolute paths to all these images.\n",
        "# image_path_set = set(image_path)\n",
        "\n",
        "\n",
        "# print(\"Images to preprocess\", len(image_path_set))\n",
        "# print(\"This make take a few minutes\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 3.19 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hb9XjL8QWzuZ",
        "colab_type": "code",
        "outputId": "5cee5f9a-df38-466d-a779-f36e6abcd0ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# import time\n",
        "# # This cell will extract activations for each image and save them to disk \n",
        "# # in NumPy format. You should not need to modify this.\n",
        "\n",
        "# # Note: we're not saving these activations to the cloud,\n",
        "# # but you certainly could if you wanted to skip this step \n",
        "# # in the future. If you look into doing that, it's best to save one large\n",
        "# # zip with the activations, and download that and extract it locally\n",
        "# # (rather than doing a bunch of network access to retrieve individual files),\n",
        "# # especially when training your model.\n",
        "# # Start time stamp\n",
        "# t = time.time()\n",
        "\n",
        "# # Create a dataset to load each image off disk, and extract activations\n",
        "# activation_dataset = tf.data.Dataset.from_tensor_slices(list(image_path_set))\n",
        "# activation_dataset = activation_dataset.map(\n",
        "#   image_to_activations, \n",
        "#   num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(32)\n",
        "\n",
        "# # Save all activations to disk in NumPy format\n",
        "# for img_batch, path_batch in activation_dataset:\n",
        "#   batch_features = image_features_extract_model(img_batch)\n",
        "#   for bf, p in zip(batch_features, path_batch):\n",
        "#     path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "#     np.save(path_of_feature, bf.numpy())  \n",
        "\n",
        "# # End time stamp\n",
        "# elapsed = time.time() - t\n",
        "# print(\"Time taken: \" + str(elapsed/60) + \" minutes\")   "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 2.83 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-Rb4DsORTui",
        "colab_type": "text"
      },
      "source": [
        "### Create lists of questions, answers, and images for your train, validation, and test set\n",
        "\n",
        "At this point, you may be reading your starter CSV directly from disk, or you may have your own data structure in memory. Since many of the methods we'll call from this point forward take lists as input, you may find it helpful to create a data structure with three lists, that correspond to the columns from your starter CSV."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoIgQUwkR3PD",
        "colab_type": "code",
        "outputId": "acb49021-58ec-4ec0-b0f3-5092bc77011a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Your code here. Populate these for your training set.\n",
        "questions_train = list(train_questions) \n",
        "answers_train = list(train_answer_numeric) \n",
        "images_train = list(train_image_path)\n",
        "\n",
        "# Your code here. Populate these for your validation set.\n",
        "questions_val = list(val_questions)\n",
        "answers_val = list(val_answer_numeric)\n",
        "images_val = list(val_image_path)\n",
        "\n",
        "# Your code here. Populate these for your test set\n",
        "questions_test = list(test_questions)\n",
        "answers_test = list(test_answer_numeric)\n",
        "images_test = list(test_image_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 10.1 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzKZT6RmWMXp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check any nan values\n",
        "count_nan = 0\n",
        "for question in questions_val:\n",
        "  if question != question:\n",
        "    count_nan = count_nan + 1\n",
        "\n",
        "print(\"Number of nan entries: \" + str(count_nan))\n",
        "for question in questions_train[:10]:\n",
        "  print(question)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTkz5C9eb1Ic",
        "colab_type": "text"
      },
      "source": [
        "### Create and fit a tokenizer\n",
        "\n",
        "Your model will use a LSTM to process the questions. First, you'll need to vectorize your text. Tokenize the questions, and limit the vocabulary size to a reasonable size (for example, the top 3,000 words). A larger size will make a higher accuracy possible, but complicate and slow your model. Reminder: fit the tokenizer only on the training questions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4myZY6zxDwJ",
        "colab_type": "code",
        "outputId": "8e2f7425-f7ae-4d0f-be84-9e255bfffd01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# You should not need to modify this code\n",
        "VOCAB_SIZE = 3000\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts(questions_train)\n",
        "\n",
        "# Note, the tokenizer's word_index will not respect VOCAB_SIZE.\n",
        "# but, that parameter will be respected in later methods,\n",
        "# (for example, when you call text_to_sequences).\n",
        "# Also note that '0' is a reserved index for padding.\n",
        "print(\"Number of unique words: \", len(tokenizer.word_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique words:  4953\n",
            "time: 223 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Gvj1ZE-fTrP",
        "colab_type": "text"
      },
      "source": [
        "### Vectorize the questions\n",
        "In this section, you will use your tokenizer to vectorize the questions using ```texts_to_sequences```. For an example of texts_to_sequences, see this [notebook](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.1-one-hot-encoding-of-words-or-characters.ipynb).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhQu6yZhfi7K",
        "colab_type": "code",
        "outputId": "3b6ec2d5-d61d-41a6-edde-7a98210add5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Use the texts_to_sequences utility to vectorize your training, \n",
        "# validation, and test questions. \n",
        "sequences_train = tokenizer.texts_to_sequences(questions_train)\n",
        "sequences_val = tokenizer.texts_to_sequences(questions_val)\n",
        "sequences_test = tokenizer.texts_to_sequences(questions_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 271 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiDIdjpY6FHO",
        "colab_type": "text"
      },
      "source": [
        "### Pad sequences\n",
        "\n",
        "In this section, you will pad the vectorized questions using ```pad_sequences```. Your maximum sequence length is a design decision, just like vocab size. Advice, start with something short, so your model trains faster (maybe, between 10 and 20).\n",
        "\n",
        "For an example of pad_sequences, see this [notebook](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.2-understanding-recurrent-neural-networks.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7KqB0-t0vci",
        "colab_type": "code",
        "outputId": "ecb24da7-1473-47d0-b5eb-45a4efc085c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# # To choose a reasonable sequence length, examine the length of all the \n",
        "# # tokenized questions in the training set (in words).\n",
        "# # Justify your choice, e.g. you should note that percentage of the \n",
        "# # training questions that fall under that length (and will not be trimmed), \n",
        "# # and the number of questions that will be.\n",
        "# token_length_train = []\n",
        "# for sequence in sequences_train:\n",
        "#   token_length_train.append(len(sequence))\n",
        "\n",
        "# plt.figure(figsize=(10, 5))\n",
        "# n_bin = 50\n",
        "# plt.hist(token_length_train, n_bin, density=True, histtype='step',\n",
        "#                            cumulative=True, label='Empirical')\n",
        "# plt.title(\"Mean: \" + str(sum(token_length_train)/len(token_length_train)))\n",
        "# plt.grid(True)\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 889 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k66XWHrPdlKA",
        "colab_type": "code",
        "outputId": "daf62e77-24ae-41e2-9ce9-ca6e3311abdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Use the pad_sequences utility to pad your training, \n",
        "# validation, and test questions.\n",
        "MAX_SEQ_LEN = 10 \n",
        "\n",
        "padded_train = tf.keras.preprocessing.sequence.pad_sequences(sequences_train, maxlen=MAX_SEQ_LEN)\n",
        "padded_val = tf.keras.preprocessing.sequence.pad_sequences(sequences_val, maxlen=MAX_SEQ_LEN)\n",
        "padded_test = tf.keras.preprocessing.sequence.pad_sequences(sequences_test, maxlen=MAX_SEQ_LEN)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 91.5 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCKOXMDwTvtE",
        "colab_type": "text"
      },
      "source": [
        "### Sanity check\n",
        "You've just done a **bunch** of preprocessing. Optionally, now would be a good time to write a block of code to verify the tokenized and padded sequences are in the format you expect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp9Wwup-T3aj",
        "colab_type": "code",
        "outputId": "dc05d78c-8f32-414e-90d6-c627376ba1af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check the tokenized set\n",
        "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "def sequence_to_text(list_of_indices):\n",
        "    # Looking up words in dictionary\n",
        "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
        "    return(words)\n",
        "\n",
        "for sequence, quest in zip(sequences_train[:10], questions_train[:10]):\n",
        "  print(str(sequence_to_text(sequence)) + \" | \" + quest)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['is', 'the', 'man', 'looking', 'for', 'food'] | is the man looking for food\n",
            "['do', 'the', 'elephants', 'want', 'to', 'hurt', 'her'] | do the elephants want to hurt her\n",
            "['are', 'all', 'these', 'people', 'watching', 'tv'] | are all these people watching tv\n",
            "['are', 'there', 'many', 'people', 'waiting', 'for', 'the', 'bus'] | are there many people waiting for the bus\n",
            "['is', 'the', 'bear', 'eating'] | is the bear eating\n",
            "['is', 'the', 'baby', 'wearing', 'pink'] | is the baby wearing pink\n",
            "['is', 'there', 'a', 'mountain', 'in', 'the', 'photo'] | is there a mountain in the photo\n",
            "['does', 'this', 'contain', 'carrots'] | does this contain carrots\n",
            "['is', 'the', 'giraffe', 'in', 'captivity'] | is the giraffe in captivity\n",
            "['are', 'the', 'glasses', 'the', 'man', 'is', 'wearing', 'used', 'for', 'reading'] | are the glasses the man is wearing used for reading\n",
            "time: 10.9 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6ppCX-tpT2J",
        "colab_type": "code",
        "outputId": "123a5727-3034-4884-d764-eb23a06217cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check the padded set\n",
        "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "\n",
        "for sequence, quest in zip(padded_train[:10], questions_train[:10]):\n",
        "  print(str(sequence_to_text(sequence)) + \" | \" + quest)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[None, None, None, None, 'is', 'the', 'man', 'looking', 'for', 'food'] | is the man looking for food\n",
            "[None, None, None, 'do', 'the', 'elephants', 'want', 'to', 'hurt', 'her'] | do the elephants want to hurt her\n",
            "[None, None, None, None, 'are', 'all', 'these', 'people', 'watching', 'tv'] | are all these people watching tv\n",
            "[None, None, 'are', 'there', 'many', 'people', 'waiting', 'for', 'the', 'bus'] | are there many people waiting for the bus\n",
            "[None, None, None, None, None, None, 'is', 'the', 'bear', 'eating'] | is the bear eating\n",
            "[None, None, None, None, None, 'is', 'the', 'baby', 'wearing', 'pink'] | is the baby wearing pink\n",
            "[None, None, None, 'is', 'there', 'a', 'mountain', 'in', 'the', 'photo'] | is there a mountain in the photo\n",
            "[None, None, None, None, None, None, 'does', 'this', 'contain', 'carrots'] | does this contain carrots\n",
            "[None, None, None, None, None, 'is', 'the', 'giraffe', 'in', 'captivity'] | is the giraffe in captivity\n",
            "['are', 'the', 'glasses', 'the', 'man', 'is', 'wearing', 'used', 'for', 'reading'] | are the glasses the man is wearing used for reading\n",
            "time: 12.6 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wocaXqRkZa_",
        "colab_type": "text"
      },
      "source": [
        "### Create a tf.dataset for training, validation, and testing\n",
        "\n",
        "The method to create the dataset is provided for you, though you will need to get it working by passing the ```padded_train```, ```answers_train``` ```and images_train``` lists you created above.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HoznUX469tT",
        "colab_type": "code",
        "outputId": "669ed9f6-f0d3-46f7-decb-ae78afccb77c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "# Load cached activations off disk.\n",
        "def load_np(img_path, question, answer):\n",
        "  img_path_str = img_path.decode('utf-8')\n",
        "  head_tail = os.path.split(img_path_str)\n",
        "  activation_path = os.path.join(\"/content\", \"activation\", head_tail[1])\n",
        "  activations = np.load(activation_path)\n",
        "  return activations, question, answer, img_path\n",
        "\n",
        "# You should not need to modify this.\n",
        "\n",
        "# This method will create a dataset that returns four elements.\n",
        "# - a batch of activations (loaded from disk)\n",
        "# - a batch of padded questions\n",
        "# - a batch of numeric answers\n",
        "# - a batch of absolute paths to the corresponding images\n",
        "def create_dataset(images, sequences, answers):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((images, \n",
        "                                                sequences, \n",
        "                                                answers))\n",
        "  # TODO jbgordon@: rewrite this to be clearer\n",
        "  # Load the cached activations off disk\n",
        "  dataset = dataset.map(lambda x, y, z: tf.numpy_function(\n",
        "      load_np, [x, y, z], [tf.float32, tf.int32, tf.float64, tf.string]),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "  # Shuffle and batch\n",
        "  dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 14.5 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAr49L-1D0pW",
        "colab_type": "code",
        "outputId": "20eb66c4-1f49-456e-bbe5-8fad99a642bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Call the above method to create train, val, and test datasets.\n",
        "# If you want to follow along with the starter code, I suggest these\n",
        "# variable names:\n",
        "train_ds = create_dataset(images_train, padded_train, answers_train)\n",
        "val_ds = create_dataset(images_val, padded_val, answers_val)\n",
        "test_ds = create_dataset(images_test, padded_test, answers_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 183 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA3D5qNfVWSs",
        "colab_type": "text"
      },
      "source": [
        "### Sanity check (optional)\n",
        "That dataset creation method is complicated. Write a block of code that demonstrates how to use the dataset (e.g., retrieve a batch of activations, questions, answeres, and images paths) and verify they look as expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS0KjlYM7UKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Verify your datasets are working properly\n",
        "\n",
        "# Here is code you can use to quickly retrieve a batch of data\n",
        "# my_iterator = iter(train_ds)\n",
        "# activations_batch, questions_batch, answers_batch, paths_batch = next(my_iterator)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lB8rjOVaUqG7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(activations_batch.shape,\n",
        "#       questions_batch.shape, \n",
        "#       answers_batch.shape, \n",
        "#       paths_batch.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z7a9VExmj_5",
        "colab_type": "text"
      },
      "source": [
        "### Define your VQA model\n",
        "\n",
        "You can base your code off the example give toward bottom of this [guide](https://keras.io/getting-started/functional-api-guide/) (but use a smaller model to start). Aim for a couple hundred thousand parameters or so"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIjyol_olOxx",
        "colab_type": "code",
        "outputId": "b8872a96-a9ec-4665-dc3b-df0cafb8c20d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Embedding, Flatten, Input, LSTM\n",
        "from tensorflow.keras.models import Model, Sequential"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.04 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUOdkiOemwPZ",
        "colab_type": "code",
        "outputId": "404a4406-d15b-44d4-cdc7-42a68140b8e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Your code here\n",
        "# Below is starter code for your model for you to complete.\n",
        "# See https://keras.io/getting-started/functional-api-guide/ for the idea.\n",
        "# The vision model is written for you. You will need to write the question\n",
        "# model.\n",
        "\n",
        "# Input to your vision model (activations from Inception-V3,\n",
        "# loaded off disk disk by the dataset above).\n",
        "image_input = Input(shape=(8, 8, 2048)) \n",
        "vision_model = Sequential()\n",
        "\n",
        "# Used to reduce the number of parameters (rather using a dense layer here).\n",
        "vision_model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
        "vision_model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "# Output of your vision model\n",
        "encoded_image = vision_model(image_input) \n",
        "\n",
        "# Your code here\n",
        "# Write your test processing model that takes the vectorized and padded\n",
        "# question as input.\n",
        "# As in the guide above, you will want to produce an `encoded_question`\n",
        "# as output\n",
        "embed_dim = 20\n",
        "question_input = Input(shape=(MAX_SEQ_LEN,), dtype='int32')\n",
        "embedded_question = Embedding(input_dim=VOCAB_SIZE, output_dim=embed_dim, \n",
        "                              input_length=MAX_SEQ_LEN)(question_input)\n",
        "encoded_question = LSTM(embed_dim)(embedded_question)\n",
        "\n",
        "# Concatenate the encoded image and question\n",
        "merged = tf.keras.layers.concatenate([encoded_image, encoded_question])\n",
        "\n",
        "# Optionally, add a small dense layer\n",
        "dropout_rate = 0.2\n",
        "regularize_rate = 0.0001\n",
        "\n",
        "dense1 = Dense(128, kernel_regularizer=tf.keras.regularizers.l1_l2(regularize_rate))(merged)\n",
        "batch_norm1 = tf.keras.layers.BatchNormalization()(dense1)\n",
        "activation1 = tf.keras.layers.Activation(\"relu\")(batch_norm1)\n",
        "dropout1 = tf.keras.layers.Dropout(dropout_rate)(activation1)\n",
        "\n",
        "# dense2 = Dense(256)(dropout1)\n",
        "# batch_norm2 = tf.keras.layers.BatchNormalization()(dense2)\n",
        "# activation2 = tf.keras.layers.Activation(\"relu\")(batch_norm2)\n",
        "# dropout2 = tf.keras.layers.Dropout(dropout_rate)(activation2)\n",
        "\n",
        "# Next, add a binary classifier on top\n",
        "output = Dense(1, activation='sigmoid')(dropout1)\n",
        "\n",
        "# Your final model\n",
        "model = Model(inputs=[image_input, question_input], outputs=output)\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 622 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc5OEmXsp4CH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cITtsogNXvgp",
        "colab_type": "text"
      },
      "source": [
        "### Plot your model\n",
        "Create a schematic that shows the graph of your model, using [plot_model](https://keras.io/visualization/). This can be helpful to ensure the vision and question paths look at expected (it's also super cool, and more informative than .summary)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPYyzzf2XxbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You should not need to modify this.\n",
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model, to_file='model.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WytdlCrO6iaq",
        "colab_type": "text"
      },
      "source": [
        "### Fit your model on a single batch\n",
        "Before training on your entire dataset, a helpful first step is to train repeatedly on a single batch, and verify the loss goes to zero. If your model is working properly, it should be able to memorize a batch of data. We will use ```model.train_on_batch``` for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hvpBQv9Z9HlH",
        "outputId": "a2026db3-fa19-4e8b-ff55-920f2efc08a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# You should not nee to modify this.\n",
        "# Retrieve a batch of data from your train dataset\n",
        "activations_batch, questions_batch, answers_batch, paths_batch = next(iter(train_ds))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.01 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "imSVGkEfaz0O",
        "colab": {}
      },
      "source": [
        "# Train them model repeatedly using model.train_on_batch\n",
        "# Verify the loss goes to zero after +/- 100 training steps.\n",
        "# If it does not, now would be a great time to debug \n",
        "# before proceeding further.\n",
        "train_steps = 50\n",
        "loss = []\n",
        "accuracy = []\n",
        "for i in range(train_steps):\n",
        "  metrics = model.train_on_batch([activations_batch, questions_batch], answers_batch)\n",
        "  loss.append(metrics[0])\n",
        "  accuracy.append(metrics[1])\n",
        "\n",
        "\n",
        "epochs = range(len(accuracy))\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "ax1 = fig.add_subplot(121)\n",
        "plt.title('Training accuracy')\n",
        "plt.plot(epochs, accuracy, color='blue', label='Train')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "ax1 = fig.add_subplot(122)\n",
        "plt.title('Training loss')\n",
        "plt.plot(epochs, loss, color='blue', label='Train')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4OYbOcUa04s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input(\"Press Enter to continue...\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02reDBJKtCk8",
        "colab_type": "text"
      },
      "source": [
        "### Use your model to make predictions on the same batch above\n",
        "Compare the predicted answer to the label. Verify they match."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaIEX9AJnRTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You should not need to modify this, but you will want to \n",
        "# carefully inspect the output.\n",
        "# for prediction, answer in zip(model.predict(x=[activations_batch, questions_batch]), answers_batch):\n",
        "#   print(np.round(prediction), answer.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDqSu9Ym04nR",
        "colab_type": "text"
      },
      "source": [
        "### Create a checkpoints directory\n",
        "\n",
        "Once you have been able to train your model on a single batch, it's time to begin training on your training dataset. It may take a while to train for a single epoch, and it would be unfortunate if Colab disconnected after training for a while, and you lost your progress.\n",
        "\n",
        "* After each training epoch, save the model's weights in a checkpoint file. You can learn more about how to create checkpoints [here](https://www.tensorflow.org/guide/keras/save_and_serialize). \n",
        "\n",
        "* To resume training, load the latest checkpoint from disk. This will restore the latest weights and resume your progress. If it does not exist, begin training from scratch.\n",
        "\n",
        "* As a tip, you may want to store your checkpoints in Google Drive, so you'll still have access to them if Colab disconnects. \n",
        "\n",
        "Note that running long jobs is not what Colab is intended for. Normally, you could simple save your checkpoints on the machine you're working on. Saving to drive adds an extra step, but it's worthwhile learning how to do.\n",
        "\n",
        "Tip: after mounting Drive, **do not** programmatically run any commands to delete files from your checkpoints folder or elsewhere (e.g., by using ```!rm -rf```) in Colab. If you're not careful, you may accidentally wipe out your entire drive if you make a programming mistake. \n",
        "\n",
        "* Instead, if you need to delete checkpoints, do so manually through the drive user interface. Note that files deleted through the UI may take a minute or so to \"actually\" be deleted as reflected by ```!ls``` commands run from Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMzFtWcJy8eC",
        "colab_type": "code",
        "outputId": "04ece8b1-b196-4780-d8ce-f15d163d93a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# If you'd like to save checkpoints in drive, you will need to uncomment\n",
        "# the code below. Alternatively, you can modify it to save\n",
        "# checkpoints in Colab (these will not persist if your instance is terminated,\n",
        "# but you can manually download them if you like).\n",
        "drive_root = '/content/drive/My Drive/'\n",
        "checkpoint_dir = os.path.join(drive_root, \"checkpoints\")\n",
        "checkpoint_dir = os.path.join(checkpoint_dir, \"hw4\")\n",
        "\n",
        "# Used for formatting\n",
        "checkpoint_path = os.path.join(checkpoint_dir, \"cp-{epoch:08d}.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 3.33 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcLFRePmatah",
        "colab_type": "code",
        "outputId": "d4d21bac-7040-4e3f-db3c-c78e3862ac9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Uncomment this if you'd like to create a checkpoints folder in your drive\n",
        "print(\"Checkpoints directory is\", checkpoint_dir)\n",
        "if os.path.exists(checkpoint_dir):\n",
        "  print(\"Checkpoints folder already exists\")\n",
        "else:\n",
        "  print(\"Creating a checkpoints directory\")\n",
        "  os.makedirs(checkpoint_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checkpoints directory is /content/drive/My Drive/checkpoints/hw4\n",
            "Creating a checkpoints directory\n",
            "time: 8.8 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF_e7Sf1awnH",
        "colab_type": "text"
      },
      "source": [
        "### Restore the latest checkpoint (if it exists)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCDBhc4XnnGJ",
        "colab_type": "code",
        "outputId": "07f33320-d2c6-495f-fd68-8ed9794f01e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# You should not need to modify this code.\n",
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "if latest != None:\n",
        "  print(\"Loading weights from\", latest)\n",
        "  model.load_weights(latest)\n",
        "else:\n",
        "  print(\"Checkpoint not found. Starting from scratch\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checkpoint not found. Starting from scratch\n",
            "time: 4.4 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVCezYfDa6n_",
        "colab_type": "text"
      },
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRVbkMtR_lp2",
        "colab_type": "code",
        "outputId": "93b0a394-d747-4c91-f6fe-97c32be3bb31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Utilities to help us record metrics.\n",
        "# You should not need to modify this code\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
        "\n",
        "val_loss = tf.keras.metrics.BinaryCrossentropy(name='val_loss')\n",
        "val_accuracy = tf.keras.metrics.BinaryAccuracy(name='val_accuracy')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 32.6 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PahaAZPgu-hV",
        "colab_type": "text"
      },
      "source": [
        "### Code to evaluate on the validation dataset\n",
        "The validation dataset may be large. It would be wasteful to evaluate on the entire validation dataset each training epoch. Instead, you could evaluate every N epochs, or, you can use the below methoid to evaluate for a fixed number of steps (batches). This will give you a noisier evaluation, but a useful indicator of how your model is doing over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mtca2G8FCF_",
        "colab_type": "code",
        "outputId": "cce5bced-ef19-451b-bcbc-e040703a45b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# You should not need to modify this.\n",
        "def evaluate(max_steps=None):\n",
        "  steps = 0\n",
        "  for activation_batch, question_batch, answer_batch, path_batch in val_ds:\n",
        "    if max_steps != None and steps == max_steps:\n",
        "      break\n",
        "    predictions = model.predict(x=[activation_batch, question_batch])\n",
        "    steps += 1 \n",
        "    # Record metrics after each batch\n",
        "    val_loss(answer_batch, predictions)\n",
        "    val_accuracy(answer_batch, predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 4.68 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpRR3fbb03z3",
        "colab_type": "text"
      },
      "source": [
        "### Train the model\n",
        "As before, we'll write our training loop using ```train_on_batch```. This is intermediate in complexity between using ```.fit``` and writing everything from scratch using a ```GradientTape```. Because we're not using ```.fit```. Since we're not using .fit, there's a bit of extra code we need to write ourselves to track loss and accuracy as we go."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e-1dK1YCR8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Used to track loss and accuracy as we go\n",
        "# You should not need to modify these\n",
        "MAX_VAL_STEP = 100\n",
        "train_loss_history, train_acc_history = [], []\n",
        "val_loss_history, val_acc_history = [], []\n",
        "\n",
        "last_epoch = -1\n",
        "epochs = 100 \n",
        "\n",
        "# Training loop\n",
        "for epoch in range(last_epoch+1, last_epoch+1+epochs):\n",
        "\n",
        "  start = time.time()\n",
        "\n",
        "  # Train for one epoch\n",
        "  batch = 0\n",
        "  for activation_batch, question_batch, answer_batch, path_batch in train_ds:\n",
        "    result = model.train_on_batch(x=[activation_batch, question_batch], y=answer_batch)\n",
        "\n",
        "    # Record metrics after each batch\n",
        "    train_loss(result[0])\n",
        "    train_accuracy(result[1])\n",
        "\n",
        "  # Evaluate for a few steps\n",
        "  evaluate(max_steps=MAX_VAL_STEP)\n",
        "\n",
        "  # Print progress\n",
        "  # You should not need to modify this.\n",
        "  template = 'Epoch {}, Loss: {:.2f}, Accuracy: {:.2f}, Val Loss {:.2f}, Val Accuracy {:.2f}, Time: {:.1f} secs'\n",
        "  print(template.format(epoch,\n",
        "                        train_loss.result(),\n",
        "                        train_accuracy.result() * 100,\n",
        "                        val_loss.result(),\n",
        "                        val_accuracy.result() * 100,\n",
        "                        time.time() - start))\n",
        "  \n",
        "  # Record history\n",
        "  train_loss_history.append(train_loss.result())\n",
        "  train_acc_history.append(train_accuracy.result() * 100)\n",
        "  val_loss_history.append(val_loss.result())\n",
        "  val_acc_history.append(val_accuracy.result() * 100)\n",
        "\n",
        "  # Reset the metrics for the next epoch\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  val_loss.reset_states()\n",
        "  val_accuracy.reset_states()\n",
        "\n",
        "  # Save a checkpoint after each epoch\n",
        "  model.save_weights(checkpoint_path.format(epoch=epoch), save_format='tf')\n",
        "  # print(\"Weights saved\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IQd0z3RwfIN",
        "colab_type": "text"
      },
      "source": [
        "### Create plots of your training and validation loss and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_UgdSUaoGND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The history object contains results on the training and test\n",
        "# sets for each epoch\n",
        "acc = train_acc_history\n",
        "val_acc = val_acc_history\n",
        "loss = train_loss_history\n",
        "val_loss = val_loss_history\n",
        "\n",
        "# Get the number of epochs\n",
        "epochs = range(len(acc))\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "ax1 = fig.add_subplot(121)\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.plot(epochs, acc, color='blue', label='Train')\n",
        "plt.plot(epochs, val_acc, color='orange', label='Val')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "ax1 = fig.add_subplot(122)\n",
        "plt.title('Training and validation loss')\n",
        "plt.plot(epochs, loss, color='blue', label='Train')\n",
        "plt.plot(epochs, val_loss, color='orange', label='Val')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzTUcK3WclOi",
        "colab_type": "text"
      },
      "source": [
        "### At this point, you have end-to-end code to train VQA model\n",
        "Now you can begin working on increasing accuracy. For this assignment, your model should at least be able to fit your training split reasonably well. \n",
        "\n",
        "Given the amount of data we're using, you do **not** need to build a model that performs well on validation to receive full credit for this asignment (I recently made the starter dataset a bit more difficult, as a result - you may need to train using more data than I want you to use for a homework assignment, so no worries)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCe18LhedPsg",
        "colab_type": "text"
      },
      "source": [
        "### Writen response\n",
        "How would you improve the accuracy on the validation set? Please list three ideas below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLUY5gMF7FzU",
        "colab_type": "text"
      },
      "source": [
        "#### TODO: your answer here\n",
        "\n",
        "1. Train certain layers of InceptionV3 network / Make custom model / Use and train on a simpler pre-made model (MobileNet, ...)\n",
        "2. Data augmentation\n",
        "3. Get more data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YisAhTTCHxd_",
        "colab_type": "text"
      },
      "source": [
        "### Finally, evaluate your model on the test set\n",
        "How well did it do? This assignment is lengthy. For our purposes, it's fine to report the accuracy - and you're done :) \n",
        "\n",
        "For a proper evaluation, take a look at this [paper](https://arxiv.org/abs/1612.00837), as discussed in class.\n",
        "\n",
        "Note: if your model is performing poorly on the test set, you can still receive full credit for this assignment as mentioned above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSQkgWG8Hyfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Restore the latest checkpoint\n",
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "assert latest != None\n",
        "model.load_weights(latest)\n",
        "print(\"loaded weights from\", latest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJhBzTEfxHLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "# Calculate accuracy on the test set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQZtYaVO5Dhe",
        "colab_type": "text"
      },
      "source": [
        "Phew, that's it! This was a long assignment, I hope it was a useful (and fun!) experience."
      ]
    }
  ]
}